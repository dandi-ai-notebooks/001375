# %% [markdown]
# # Exploring Dandiset 001375: Septum GABA disruption with DREADDs

# %% [markdown]
# **Disclaimer:** This notebook was primarily generated by an AI assistant and has not been rigorously verified by human experts. Please use caution when interpreting the code and results. Verify any findings independently before drawing strong conclusions.

# %% [markdown]
# ## Overview of the Dandiset
#
# This notebook explores Dandiset [001375](https://dandiarchive.org/dandiset/001375/draft), titled "Septum GABA disruption with DREADDs".
#
# **Description:** Pilot study of the effect of disrupting septal gabaergic activity using DREADDs on hippocampal and neocortical activity.
#
# **Citation:** Eckert, Michael; McNaughton, Bruce; Ferbinteanu, Janina (2025) Septum GABA disruption with DREADDs (Version draft) [Data set]. DANDI Archive. https://dandiarchive.org/dandiset/001375/draft
#
# This notebook demonstrates how to:
# *   Load basic information about the Dandiset using the DANDI API (conceptually).
# *   Identify and access NWB files within the Dandiset.
# *   Load and explore the contents of a specific NWB file remotely using `pynwb` and `remfile`.
# *   Visualize subsets of data including trial information, electrode locations, unit spike times (raster plot), and raw electrophysiology traces.

# %% [markdown]
# ## Required Packages
#
# This notebook requires the following Python packages. It assumes they are already installed in your environment.
#
# *   `dandi` (for interacting with the DANDI Archive REST API)
# *   `pynwb` (for reading NWB files)
# *   `h5py` (NWB backend for HDF5 files)
# *   `remfile` (for accessing remote HDF5 files efficiently)
# *   `numpy` (for numerical operations)
# *   `matplotlib` (for plotting)
# *   `pandas` (for tabular data handling, used by pynwb `.to_dataframe()`)
# *   `seaborn` (for plot styling)

# %%
import pynwb
import h5py
import remfile
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

# Apply seaborn styling for general plots
sns.set_theme()

# %% [markdown]
# ## Loading Dandiset Information (Conceptual Example)
#
# While we won't run this live in the notebook to avoid potential API rate limits or redundant calls, here's how you would typically use the `dandi` library to get information about the Dandiset. We have already gathered this information externally for use in this notebook.

# %%
# Conceptual code - not executed in this notebook
# from dandi.dandiapi import DandiAPIClient

# dandiset_id = "001375"
# version = "draft" # Or a specific published version

# try:
#     with DandiAPIClient() as client:
#         dandiset = client.get_dandiset(dandiset_id, version)
#         metadata = dandiset.get_raw_metadata()
#         print(f"Dandiset Name: {metadata['name']}")
#         print(f"Dandiset Description: {metadata.get('description', 'N/A')}")
#         print(f"Dandiset URL: {metadata['url']}")

#         assets = list(dandiset.get_assets())
#         print(f"\nFound {len(assets)} assets.")
#         # print("First 5 assets:")
#         # for asset in assets[:5]:
#         #     print(f"- {asset.path} (ID: {asset.asset_id})")
# except Exception as e:
#      print(f"Could not fetch Dandiset info: {e}")
#      print("Using pre-gathered information instead.")

# Pre-gathered information:
print("Using pre-gathered information for Dandiset 001375 (draft):")
print("Name: Septum GABA disruption with DREADDs")
print("Description: Pilot study of the effect of disrupting septal gabaergic activity using DREADDs on hippocampal and neocortical activity")
print("URL: https://dandiarchive.org/dandiset/001375/draft")
print("Assets found: 3")
print("Example Asset Path: sub-MS13B/sub-MS13B_ses-20240725T190000_ecephys.nwb")


# %% [markdown]
# ## Loading a Specific NWB File
#
# We will now load one of the NWB files from this Dandiset. We'll use the file:
# `sub-MS13B/sub-MS13B_ses-20240725T190000_ecephys.nwb`
#
# This file corresponds to Asset ID `ce525828-8534-4b56-9e47-d2a34d1aa897`.
#
# We construct the download URL for the asset and use `remfile` and `pynwb` to open it remotely.

# %%
asset_id = "ce525828-8534-4b56-9e47-d2a34d1aa897"
asset_path = "sub-MS13B/sub-MS13B_ses-20240725T190000_ecephys.nwb"
dandiset_id = "001375"
dandiset_version = "draft"

# Construct the DANDI API URL for the asset download
nwb_url = f"https://api.dandiarchive.org/api/assets/{asset_id}/download/"
print(f"Loading NWB file from: {nwb_url}")

# Use remfile to access the remote HDF5 file efficiently
print("Opening remote file handle...")
remote_file = remfile.File(nwb_url)
h5_file = h5py.File(remote_file)

print("Reading NWB file structure...")
# Important: Set load_namespaces=True for compatibility with newer NWB standards
io = pynwb.NWBHDF5IO(file=h5_file, mode='r', load_namespaces=True)
nwb = io.read()

print("NWB file loaded successfully.")
print(f"\nSession Description: {nwb.session_description}")
print(f"Identifier: {nwb.identifier}")
print(f"Session Start Time: {nwb.session_start_time}")
print(f"Subject ID: {nwb.subject.subject_id if nwb.subject else 'N/A'}")
print(f"Subject Species: {nwb.subject.species if nwb.subject else 'N/A'}")


# %% [markdown]
# ### Neurosift Link
#
# You can explore this NWB file interactively using Neurosift:
# [Explore on Neurosift](https://neurosift.app/nwb?url={nwb_url}&dandisetId={dandiset_id}&dandisetVersion={dandiset_version})

# %% [markdown]
# ## Exploring File Contents
#
# Let's look at some of the data structures available in this NWB file.

# %% [markdown]
# ### Trials Information
#
# The file contains information about experimental trials, stored in `nwb.intervals['trials']`. We can view this as a table.

# %%
if 'trials' in nwb.intervals:
    trials_df = nwb.intervals['trials'].to_dataframe()
    print(f"Found {len(trials_df)} trials.")
    print("First 5 trials:")
    print(trials_df.head())

    # Calculate trial durations
    if 'start_time' in trials_df.columns and 'stop_time' in trials_df.columns:
        trial_durations = trials_df['stop_time'] - trials_df['start_time']
        plt.figure(figsize=(10, 4))
        plt.hist(trial_durations, bins=30)
        plt.title('Distribution of Trial Durations')
        plt.xlabel('Duration (s)')
        plt.ylabel('Number of Trials')
        plt.show()
    else:
        print("Could not calculate trial durations (missing start/stop times).")
else:
    print("No 'trials' interval table found in the NWB file.")


# %% [markdown]
# ### Electrode Information
#
# The file contains information about the electrodes used for recording, including their locations. This is stored in `nwb.electrodes`.

# %%
if nwb.electrodes is not None:
    electrodes_df = nwb.electrodes.to_dataframe()
    print(f"Found {len(electrodes_df)} electrodes.")
    print("Columns available:", list(electrodes_df.columns))
    print("\nFirst 5 electrodes:")
    print(electrodes_df.head())

    # Plot electrode locations if x, y, and group information is available
    if 'x' in electrodes_df.columns and 'y' in electrodes_df.columns and 'group_name' in electrodes_df.columns:
        plt.figure(figsize=(8, 6))
        # Using seaborn's scatterplot for easy grouping by color
        sns.scatterplot(data=electrodes_df, x='x', y='y', hue='group_name', s=50) # Removed style='group_name' as it might be redundant with hue
        plt.title('Electrode Locations by Group')
        plt.xlabel('X Coordinate (um)') # Assuming um, NWB doesn't enforce unit here
        plt.ylabel('Y Coordinate (um)') # Assuming um
        plt.legend(title='Electrode Group', bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.grid(True, linestyle='--', alpha=0.6)
        plt.axis('equal') # Ensure aspect ratio is equal
        plt.show()
    else:
        print("\nCould not plot electrode locations (missing x, y, or group_name columns).")
else:
    print("No electrode information found in the NWB file.")

# %% [markdown]
# ### Units (Spike Times)
#
# The file contains spike sorting results, with spike times for detected units stored in `nwb.units`. Let's visualize the activity of a few units using a raster plot over a short time interval.

# %%
if nwb.units is not None and 'spike_times' in nwb.units.colnames:
    units_df = nwb.units.to_dataframe()
    num_units = len(units_df)
    print(f"Found {num_units} units.")
    # print("Units table columns:", list(units_df.columns))
    # print("\nFirst 5 units info:")
    # print(units_df.head()) # Displaying the dataframe helps see available info, though spike_times are ragged

    # --- Raster Plot ---
    # Define time window for the raster plot (e.g., first 10 seconds)
    t_start = 0.0
    t_end = 10.0

    # Select a subset of units to plot (e.g., first 10 or max 20)
    max_units_to_plot = min(num_units, 20)
    unit_indices_to_plot = list(range(max_units_to_plot))
    unit_ids_to_plot = nwb.units.id[unit_indices_to_plot] # Get actual unit IDs

    plt.figure(figsize=(12, 6))
    event_times = []
    unit_plot_indices = []

    print(f"\nGenerating raster plot for first {max_units_to_plot} units in time range [{t_start}s, {t_end}s]...")

    # Access spike times using the VectorIndex and VectorData paradigm
    spike_times_indexer = nwb.units['spike_times'] # This is the VectorIndex object
    all_spike_times_data = nwb.units['spike_times'].data # This is the VectorData (flattened array)

    for i, unit_idx in enumerate(unit_indices_to_plot):
        # Get start/end indices for this unit's spikes from the indexer's data
        start_idx = spike_times_indexer.data[unit_idx]
        # The end index for unit N is the start index for unit N+1
        # For the last unit, the end index is the total number of spikes in the VectorData
        end_idx = spike_times_indexer.data[unit_idx + 1] if (unit_idx + 1) < len(spike_times_indexer.data) else len(all_spike_times_data)

        # Slice the flattened data array to get spikes for the current unit
        unit_spike_times = all_spike_times_data[start_idx:end_idx]

        # Filter spikes within the desired time window
        spikes_in_window = unit_spike_times[(unit_spike_times >= t_start) & (unit_spike_times <= t_end)]
        event_times.append(spikes_in_window)

        # Assign y-value for plotting (using index i for spacing)
        unit_plot_indices.append(np.full(len(spikes_in_window), i))

    # Flatten lists for plt.eventplot
    all_event_times = np.concatenate(event_times) if len(event_times) > 0 else []
    all_unit_indices = np.concatenate(unit_plot_indices) if len(unit_plot_indices) > 0 else []

    if len(all_event_times) > 0:
      plt.eventplot(all_event_times, lineoffsets=all_unit_indices, linelengths=0.75, colors='black')
      plt.yticks(ticks=range(max_units_to_plot), labels=unit_ids_to_plot) # Use actual unit IDs for labels
      plt.xlabel('Time (s)')
      plt.ylabel('Unit ID')
      plt.title(f'Raster Plot ({t_start}s - {t_end}s)')
      plt.xlim(t_start, t_end)
      plt.ylim(-1, max_units_to_plot) # Adjust ylim for better spacing
      plt.grid(True, axis='x', linestyle='--', alpha=0.6)
      plt.show()
    else:
        print(f"No spikes found for the selected units in the time window [{t_start}s, {t_end}s].")

else:
    print("No units or spike times found in the NWB file.")


# %% [markdown]
# ### Raw Electrophysiology Data
#
# The file contains raw electrophysiology data, likely in `nwb.acquisition`. Let's load and visualize a small snippet of this data from a few channels.
#
# **Note:** Loading large amounts of raw ephys data over the network can be slow and memory-intensive. We will only load a very short time segment (e.g., 0.1 seconds) for a few selected channels. This raw data requires significant processing (filtering, spike sorting) for detailed analysis, which is beyond the scope of this introductory notebook.

# %%
time_series_name = None
# Common names for raw ephys data in NWB acquisition
possible_ts_names = ['ElectricalSeries', 'time_series', 'ephys_raw'] # Added 'time_series' based on tools_cli output

for name in possible_ts_names:
    if name in nwb.acquisition:
        time_series_name = name
        break

if time_series_name and isinstance(nwb.acquisition[time_series_name], pynwb.ecephys.ElectricalSeries):
    ts = nwb.acquisition[time_series_name]
    print(f"Found ElectricalSeries: '{time_series_name}'")
    print(f"Data shape: {ts.data.shape}") # (timepoints, channels)
    print(f"Sampling rate: {ts.rate} Hz")
    print(f"Duration: {ts.data.shape[0] / ts.rate:.2f} s")
    print(f"Data type: {ts.data.dtype}")
    print(f"Unit: {ts.unit}")

    # --- Plot Snippet ---
    num_channels = ts.data.shape[1]
    sampling_rate = ts.rate

    # Select a few channels to plot (e.g., first 5)
    num_channels_to_plot = min(num_channels, 5)
    channel_indices = list(range(num_channels_to_plot))
    # Try to get actual electrode IDs/labels if available and linked
    electrode_labels = []
    # Check if electrodes table link exists and matches channel count
    if ts.electrodes is not None and len(ts.electrodes.data) == num_channels:
        try:
            linked_electrodes_df = nwb.electrodes.to_dataframe().loc[ts.electrodes.data[:]]
            # Attempt to find a meaningful label column
            label_col = None
            if 'label' in linked_electrodes_df.columns: label_col = 'label'
            elif 'group_name' in linked_electrodes_df.columns: label_col = 'group_name' # Fallback
            
            if label_col:
                 electrode_labels = [f"Ch {linked_electrodes_df.index[idx]} ({linked_electrodes_df[label_col].iloc[idx]})" for idx in channel_indices]
            else: # Fallback if no good label column found
                 electrode_labels = [f"Ch {linked_electrodes_df.index[idx]}" for idx in channel_indices]
        except Exception as e:
            print(f"Could not get detailed electrode labels: {e}. Using simple indices.")
            electrode_labels = [f"Channel {i}" for i in channel_indices]
            
    else: # Fallback if electrodes are not linked or mismatch
        electrode_labels = [f"Channel {i}" for i in channel_indices]


    # Define time window (e.g., 0.1 seconds starting at 1s)
    start_time_sec = 1.0
    duration_sec = 0.1
    start_index = int(start_time_sec * sampling_rate)
    end_index = start_index + int(duration_sec * sampling_rate)

    # Ensure indices are within bounds
    if start_index >= 0 and end_index <= ts.data.shape[0] and start_index < end_index:
        print(f"\nLoading data snippet: Time {start_time_sec:.2f}s - {start_time_sec + duration_sec:.2f}s, Channels {channel_indices}...")

        # Load the data subset: shape (timepoints_subset, channels_subset)
        # Important: Use slicing to load only the required data into memory
        try:
            data_snippet = ts.data[start_index:end_index, channel_indices]

            # Apply conversion factor if available (often needed for Volts/mV)
            conversion_factor = ts.conversion if hasattr(ts, 'conversion') and ts.conversion != 0 else 1.0
            offset = ts.offset if hasattr(ts, 'offset') else 0.0
            data_snippet_converted = (data_snippet.astype(np.float32) * conversion_factor) + offset
            plot_unit = ts.unit

            time_vector = np.linspace(start_time_sec, start_time_sec + duration_sec, num=data_snippet.shape[0])

            plt.figure(figsize=(14, 8))
            # Plot each channel trace with an offset for visibility
            offset_scale = np.ptp(data_snippet_converted[:,0]) if data_snippet_converted.shape[1] > 0 else 1.0 # Estimate scale from first channel
            offset_val = offset_scale * 1.5 # Adjust multiplier as needed

            # No seaborn styling for this type of plot
            with plt.style.context('default'):
                for i in range(num_channels_to_plot):
                    plt.plot(time_vector, data_snippet_converted[:, i] + i * offset_val, label=electrode_labels[i])

                plt.title(f'Raw Ephys Snippet ({duration_sec}s)')
                plt.xlabel('Time (s)')
                plt.ylabel(f'Signal ({plot_unit}, offset added)')
                plt.yticks([]) # Hide Y-axis ticks as offset makes them meaningless
                plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
                plt.grid(True, linestyle='--', alpha=0.6)
                plt.tight_layout(rect=[0, 0, 0.85, 1]) # Adjust layout to make space for legend
                plt.show()

        except Exception as e:
            print(f"\nError loading or plotting ephys data snippet: {e}")
            print("This might be due to data access issues or large data size.")

    else:
        print(f"\nChosen time window [{start_time_sec}s, {start_time_sec + duration_sec}s] is out of bounds for the data (duration {ts.data.shape[0] / ts.rate:.2f}s).")

elif time_series_name:
     print(f"Found object '{time_series_name}' in acquisition, but it is not an ElectricalSeries.")
else:
    print("No suitable ElectricalSeries found in nwb.acquisition.")


# %% [markdown]
# ## Summary and Future Directions
#
# This notebook demonstrated how to access and explore Dandiset 001375, focusing on a single NWB file (`sub-MS13B/..._ecephys.nwb`). We covered:
# *   Retrieving basic Dandiset metadata (conceptually).
# *   Loading an NWB file remotely using `pynwb`, `h5py`, and `remfile`.
# *   Inspecting and visualizing key data components:
#     *   Trial structure (start/stop times).
#     *   Electrode layout.
#     *   Unit spike times via a raster plot for a subset of units and time.
#     *   A small snippet of raw electrophysiology traces for a subset of channels.
#
# **Key Takeaways:**
# *   The data includes trial information, electrode positions, sorted unit spike times, and raw voltage traces.
# *   Accessing data remotely is feasible, but care must be taken when loading large datasets like raw ephys traces; loading subsets is crucial.
#
# **Future Directions:**
# *   Analyze trial-related activity: Align neural activity (spikes) to trial start/stop times.
# *   Perform more detailed analysis of spike train statistics (e.g., firing rates, inter-spike intervals).
# *   If LFP data is available in other acquisition objects, explore LFP signals and their relationship to behavior or spikes.
# *   Apply standard preprocessing steps (filtering, referencing) to the raw ephys data before further analysis (computationally intensive).
# *   Compare data across different sessions or subjects within the Dandiset (requires loading other NWB files).
# *   Investigate the specific effects of the DREADD manipulation mentioned in the Dandiset description, potentially by comparing conditions if available within the data structure (e.g., in `nwb.intervals['trials']` columns).

# %% [markdown]
# *End of Notebook*

# %%
# Close the NWB file reader and the underlying HDF5 file
# It's good practice to close the file when done, especially for remote files
try:
    io.close()
    # remfile does not explicitly need closing in this context,
    # but ensuring the h5py file object linked to it is closed is key.
    print("\nNWB file IO closed.")
except Exception as e:
    print(f"Error closing NWB file IO: {e}")