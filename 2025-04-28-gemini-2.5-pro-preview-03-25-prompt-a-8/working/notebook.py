# %% [markdown]
# # Exploring Dandiset 001375: Septum GABA disruption with DREADDs
#
# **Disclaimer:** This notebook was primarily generated by an AI assistant and has not been thoroughly verified by human experts. Please exercise caution when interpreting the code and results. Independent verification is recommended.
#
# ## Overview
#
# This notebook explores Dandiset [001375](https://dandiarchive.org/dandiset/001375/draft) from the DANDI Archive.
#
# **Description from Dandiset:** Pilot study of the effect of disrupting septal gabaergic activity using DREADDs on hippocampal and neocortical activity.
#
# **Contributors:** Eckert, Michael; NIH Brain; McNaughton, Bruce; Ferbinteanu, Janina
#
# This notebook demonstrates how to:
# * Load the Dandiset metadata using the DANDI API.
# * List the assets (files) within the Dandiset.
# * Select and load an NWB (Neurodata Without Borders) file from the Dandiset.
# * Examine the metadata and structure of the NWB file.
# * Access and visualize electrophysiology data (specifically, a segment of the raw time series) from the NWB file.
# * Access spike times for identified units.

# %% [markdown]
# ## Required Packages
#
# The following Python packages are required to run this notebook. It is assumed that these packages are already installed.
#
# * `dandi` (for interacting with the DANDI Archive)
# * `pynwb` (for reading NWB files)
# * `h5py` (dependency for pynwb)
# * `remfile` (for streaming remote files)
# * `numpy` (for numerical operations)
# * `matplotlib` (for plotting)
# * `seaborn` (for enhanced plotting styles)
# * `pandas` (for data manipulation, especially with tables)

# %%
import pynwb
import h5py
import remfile
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from dandi.dandiapi import DandiAPIClient

print('Libraries imported successfully.')

# %% [markdown]
# ## Loading Dandiset Information
#
# We will use the DANDI Python client to connect to the archive and retrieve information about the Dandiset.

# %%
# Connect to DANDI archive
client = DandiAPIClient()

# Get the Dandiset object
dandiset_id = "001375"
dandiset = client.get_dandiset(dandiset_id)

# Print basic information about the Dandiset
metadata = dandiset.get_raw_metadata()
print(f"Dandiset name: {metadata['name']}")
print(f"Dandiset URL: {metadata['url']}")
print(f"Dandiset description: {metadata.get('description', 'N/A')}") # Use .get for robustness

# List the assets in the Dandiset
assets = list(dandiset.get_assets())
print(f"\nFound {len(assets)} assets in the dataset")
print("\nAssets:")
for asset in assets:
    print(f"- {asset.path} (ID: {asset.identifier}, Size: {asset.size} bytes)")

# %% [markdown]
# ## Accessing an NWB File
#
# We will select one of the NWB files from the asset list and load it using `pynwb`. We'll use the first asset found: `sub-MS13B/sub-MS13B_ses-20240725T190000_ecephys.nwb`.
#
# The `remfile` library allows us to stream the remote HDF5 file without downloading it entirely.

# %%
# Select the first asset (adjust index if needed)
selected_asset = assets[0]
print(f"Selected asset path: {selected_asset.path}")
print(f"Selected asset ID: {selected_asset.identifier}")

# Construct the asset URL
asset_url = f"https://api.dandiarchive.org/api/assets/{selected_asset.identifier}/download/"
print(f"Asset URL: {asset_url}")

# Load the NWB file using remfile and pynwb
print("Loading remote NWB file (this might take a moment)...")
remote_file = remfile.File(asset_url)
h5_file = h5py.File(remote_file)
io = pynwb.NWBHDF5IO(file=h5_file, load_namespaces=True) # Ensure namespaces are loaded
nwb = io.read()
print("NWB file loaded successfully.")

# %% [markdown]
# ## Exploring NWB File Metadata and Structure
#
# Now that the NWB file is loaded, let's examine some of its basic metadata and structure.

# %%
print(f"Session Description: {nwb.session_description}")
print(f"Identifier: {nwb.identifier}")
print(f"Session Start Time: {nwb.session_start_time}")
print(f"Subject ID: {nwb.subject.subject_id if nwb.subject else 'N/A'}")
print(f"Subject Age: {nwb.subject.age if nwb.subject else 'N/A'}")
print(f"Subject Species: {nwb.subject.species if nwb.subject else 'N/A'}")
print(f"Subject Sex: {nwb.subject.sex if nwb.subject else 'N/A'}")

# %% [markdown]
# ### NWB File Contents
#
# An NWB file organizes data into different groups. Let's list the main top-level groups available in this file:
#
# ```
# NWB File Structure (Key Components):
# ├── acquisition (Raw acquired data)
# │   └── time_series (ElectricalSeries)
# ├── processing (Processed data modules)
# │   └── (Potentially analysis results, depending on the file)
# ├── intervals (Time intervals, e.g., trials)
# │   └── trials (TimeIntervals)
# ├── units (Information about sorted units/neurons)
# │   └── (Units table)
# ├── electrodes (Information about electrodes used)
# │   └── (DynamicTable)
# ├── electrode_groups (Grouping of electrodes, e.g., by shank)
# │   ├── shank1 (ElectrodeGroup)
# │   └── shank2 (ElectrodeGroup)
# ├── devices (Information about experimental devices)
# │   └── silicon probe array (Device)
# ├── General Metadata
# │   ├── session_description
# │   ├── identifier
# │   ├── session_start_time
# │   └── subject (Subject details)
# ... (other standard NWB fields)
# ```
#
# We can access these groups directly using the `nwb` object.

# %% [markdown]
# ### Neurosift Link
#
# You can explore this NWB file interactively using Neurosift:
# [https://neurosift.app/nwb?url={asset_url}&dandisetId={dandiset_id}&dandisetVersion=draft](https://neurosift.app/nwb?url={asset_url}&dandisetId={dandiset_id}&dandisetVersion=draft)

# %% [markdown]
# ### Electrodes Table
#
# The `nwb.electrodes` table contains information about each recording channel. Let's display the first few rows.

# %%
if nwb.electrodes is not None:
    print("Electrodes table info:")
    print(f"  Number of electrodes: {len(nwb.electrodes.id[:])}")
    print(f"  Column names: {nwb.electrodes.colnames}")
    # Display the table as a pandas DataFrame
    electrodes_df = nwb.electrodes.to_dataframe()
    print("\nFirst 5 rows of the electrodes table:")
    print(electrodes_df.head())
else:
    print("No electrodes table found in this file.")

# %% [markdown]
# ### Trials Information
#
# The `nwb.intervals['trials']` object stores information about experimental trials, typically start and stop times.

# %%
if 'trials' in nwb.intervals:
    trials_table = nwb.intervals['trials']
    print("Trials table info:")
    print(f"  Description: {trials_table.description}")
    print(f"  Number of trials: {len(trials_table.id[:])}")
    print(f"  Column names: {trials_table.colnames}")
    trials_df = trials_table.to_dataframe()
    print("\nFirst 5 rows of the trials table:")
    print(trials_df.head())
else:
    print("No trials interval table found in this file.")

# %% [markdown]
# ## Visualizing Electrophysiology Data
#
# The raw electrophysiology data is often stored in `nwb.acquisition`. In this file, it seems to be under `nwb.acquisition['time_series']`. This is an `ElectricalSeries` object containing voltage recordings over time.
#
# The full dataset is very large (`(144675584, 256)` samples x channels). We will load and plot only a small time segment from a few channels to demonstrate how to access the data.

# %%
# Set seaborn style for plots (excluding image plots)
sns.set_theme()

if 'time_series' in nwb.acquisition:
    ephys_series = nwb.acquisition['time_series']
    print("Electrical Series Info:")
    print(f"  Data shape: {ephys_series.data.shape}")
    print(f"  Sampling rate: {ephys_series.rate} Hz")
    print(f"  Unit: {ephys_series.unit}")

    # Define parameters for data subset
    start_time_sec = 10.0  # Start time in seconds
    duration_sec = 0.5     # Duration to load in seconds
    num_channels_to_plot = 4 # Number of channels to plot

    # Calculate sample indices
    sampling_rate = ephys_series.rate
    start_sample = int(start_time_sec * sampling_rate)
    end_sample = int((start_time_sec + duration_sec) * sampling_rate)

    # Ensure indices are within bounds
    if start_sample < 0: start_sample = 0
    if end_sample > ephys_series.data.shape[0]: end_sample = ephys_series.data.shape[0]
    if start_sample >= end_sample:
         print(f"Error: Calculated start sample ({start_sample}) >= end sample ({end_sample}). Cannot load data.")
    else:
        # Select channel indices
        total_channels = ephys_series.data.shape[1]
        channel_indices = np.linspace(0, total_channels - 1, num_channels_to_plot, dtype=int)

        print(f"\nLoading data from {start_time_sec:.2f}s to {start_time_sec + duration_sec:.2f}s ({duration_sec}s duration)")
        print(f"Sample range: {start_sample} to {end_sample}")
        print(f"Loading data for channels: {channel_indices}")

        # Load data subset (samples, channels)
        # Important: Use slicing [:] to load data into memory as NumPy array
        try:
            data_subset = ephys_series.data[start_sample:end_sample, channel_indices]
             # Convert to mV if necessary (check unit and conversion factor)
            conversion_factor = ephys_series.conversion if ephys_series.conversion != 0.0 else 1.0
            offset = ephys_series.offset if ephys_series.offset != 0.0 else 0.0
            if ephys_series.unit == 'volts' or ephys_series.unit == 'V':
                 data_subset_mv = (data_subset * conversion_factor + offset) * 1000 # Convert V to mV
                 plot_unit = 'mV'
            elif ephys_series.unit == 'millivolts' or ephys_series.unit == 'mV':
                 data_subset_mv = data_subset * conversion_factor + offset
                 plot_unit = 'mV'
            else:
                 # Assume microvolts or other - plot as is, adjust unit label
                 data_subset_mv = data_subset * conversion_factor + offset
                 plot_unit = ephys_series.unit # Use the unit from the file


            # Create time vector for the subset
            time_vector_sec = np.arange(start_sample, end_sample) / sampling_rate

            # Plotting
            plt.figure(figsize=(15, 6))
            offset_plot = 0
            # Calculate a reasonable offset based on data range
            data_range = np.ptp(data_subset_mv) if data_subset_mv.size > 0 else 1.0
            y_offset = data_range * 1.1 # Add 10% gap

            for i, ch_index in enumerate(channel_indices):
                plt.plot(time_vector_sec, data_subset_mv[:, i] + offset_plot, label=f'Channel {ch_index}')
                offset_plot += y_offset # Add offset for next channel

            plt.title(f'Raw Ephys Data Snippet ({duration_sec}s)')
            plt.xlabel('Time (s)')
            plt.ylabel(f'Voltage ({plot_unit}) + Offset')
            plt.legend(loc='upper right')
            plt.grid(True, linestyle=':')
            plt.tight_layout()
            plt.show()

        except Exception as e:
            print(f"Error loading or plotting ephys data: {e}")
            print("This could be due to issues with indexing, data size, or network streaming.")

else:
    print("No 'time_series' found in nwb.acquisition.")


# %% [markdown]
# ## Accessing Unit Spike Times
#
# The `nwb.units` table contains information about detected units (neurons), primarily their spike times.

# %%
if nwb.units is not None:
    print("Units table info:")
    print(f"  Number of units: {len(nwb.units.id[:])}")
    print(f"  Column names: {nwb.units.colnames}")

    # Get the units table as a DataFrame for easier inspection
    units_df = nwb.units.to_dataframe()
    print("\nFirst 5 rows of the units table:")
    print(units_df.head()) # Note: Spike times are often ragged arrays

    # Access spike times for a specific unit
    unit_index_to_show = 0 # Example: Show spike times for the first unit
    if len(nwb.units.id[:]) > unit_index_to_show:
        unit_id = nwb.units.id[unit_index_to_show]
        print(f"\nAccessing spike times for unit ID: {unit_id} (index {unit_index_to_show})")

        # Important: Access spike times for ONE unit using the index
        # nwb.units['spike_times'][unit_index_to_show] should work directly
        # Or using the VectorIndex approach if needed:
        try:
            # spike_times_for_unit = nwb.units['spike_times'][unit_index_to_show] # Direct access

            # Using VectorIndex: Get start/end indices for this unit's spikes
            spike_times_vecdata = nwb.units['spike_times'].vector_data
            if unit_index_to_show == 0:
                 start_idx = 0
            else:
                 start_idx = nwb.units['spike_times'].vector_index[unit_index_to_show - 1]
            end_idx = nwb.units['spike_times'].vector_index[unit_index_to_show]

            # Slice the underlying data VectorData
            spike_times_for_unit = spike_times_vecdata[start_idx:end_idx]


            num_spikes = len(spike_times_for_unit)
            print(f"  Number of spikes for unit {unit_id}: {num_spikes}")

            # Print the first few spike times
            print(f"  First 10 spike times (seconds): {np.round(spike_times_for_unit[:10], 4)}")

            # Optional: Plot spike times as a raster for the first few seconds
            plot_duration_sec = 20 # Plot spikes within the first 20 seconds
            spikes_in_window = spike_times_for_unit[spike_times_for_unit < plot_duration_sec]

            plt.figure(figsize=(15, 1))
            plt.eventplot(spikes_in_window, color='black', linelengths=0.75)
            plt.title(f'Spike Raster for Unit {unit_id} (First {plot_duration_sec} Seconds)')
            plt.xlabel('Time (s)')
            plt.yticks([]) # Hide y-axis as it's just one unit
            plt.xlim(0, plot_duration_sec)
            plt.tight_layout()
            plt.show()

        except Exception as e:
            print(f"Could not access or plot spike times for unit {unit_id}: {e}")
            print("This might happen if the spike times structure is different or indexing fails.")

    else:
         print(f"Unit index {unit_index_to_show} is out of bounds.")

else:
    print("No units table found in this file.")


# %% [markdown]
# ## Summary and Future Directions
#
# This notebook demonstrated how to programmatically access Dandiset 001375 using the DANDI API and explore the contents of one of its NWB files (`sub-MS13B/sub-MS13B_ses-20240725T190000_ecephys.nwb`). We covered:
# * Fetching Dandiset metadata and listing assets.
# * Loading a remote NWB file using `pynwb` and `remfile`.
# * Inspecting key metadata fields and data structures like electrodes and trials tables.
# * Accessing and visualizing a small segment of the raw extracellular electrophysiology time series data.
# * Accessing spike times for identified units.
#
# ### Future Directions
#
# This initial exploration opens up several possibilities for further analysis:
# * **Analyze Across Trials:** Correlate neural activity (e.g., firing rates of specific units) with behavioral events defined in the `trials` table.
# * **Explore Other Assets:** Apply similar loading and visualization techniques to the other NWB files in the Dandiset (e.g., `sub-MS14A`) to compare across subjects or sessions.
# * **Spike Train Analysis:** Perform more detailed analyses on the `units` data, such as calculating firing rates, inter-spike intervals (ISIs), or correlating activity between different units.
# * **LFP Analysis:** If Local Field Potential (LFP) data is available (often requires filtering the raw data or checking for a dedicated LFP `ElectricalSeries`), analyze oscillatory activity in different frequency bands (e.g., theta, gamma). The current file only shows raw time series, so LFP would need to be derived or found elsewhere.
# * **Relate Activity to Electrode Location:** Use the information in the `electrodes` table (location, group) to understand the spatial distribution of neural activity across different brain regions (e.g., hippocampus vs. visual cortex).
#
# Remember to consult the Dandiset documentation and associated publications for more context on the experimental design and data collection methods.

# %% [markdown]
# ---
# *End of Notebook*